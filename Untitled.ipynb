{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3949fce7",
   "metadata": {},
   "source": [
    "What is a fully connected network (FCN for short)?\n",
    "\n",
    "A fully connected network is a type of artifical neural network whose structure consists of layers connected to other layers by the neurons/nodes in each layer. For example, if there is a FCN that has 3 layers with layer 1 having 10 neurons, layer 2 having 26 neurons, and layer 3 having 14 neurons. In this example, each one of the neurons in layer 1 has a connection to every single neuron in layer 2 and the same concept holds true for the relation between layer 2 and layer 3. What is also interesting is that you can figure out the amount of connections that are there between two layers. The equation for that the N1*(N1*N2) where N1 is the number of neurons in the layer on the left and N2 is the number of neurons on the right.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3188284",
   "metadata": {},
   "source": [
    "How do Fully Connected Networks work?\n",
    "\n",
    "FCNs work by having a neuron/perceptron apply a linear transformation to the input vector through a weight matrix. Then a non-linear transformation is applied to the product of the input vector and weight matrix through a non linear activation function (show equation image).\n",
    "\n",
    "Basically, we are taking the dot product of the weight matrix W and the input vector x. Then the bias term W0 will be added inside the non linear function. (A Bias term is a disproportionate weight in favor or against an idea or thing). In even simpler terms, we are doing vector multiplication. For example, we have an input vector of 1x9 and a weight matrix of 9x4. We will take the dot product of (1x9) and (9x4) and then apply the non-linear transformation with the activation function f to get an output vector of (1x4) (show the second image before the example and the third fcl image after the example)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66e36a85",
   "metadata": {},
   "source": [
    "How does a FCN differ from a CNN?\n",
    "\n",
    "The biggest difference from what I seen online in doing my own research is that FCNs are structurally agnostic meaning that they don't make any special assumption about the input given whereas a CNN is designed to assume that the input are specifically images. \n",
    "\n",
    "This broad assumption that FCNs have can be quite useful if one wants to train different data. However, due to the broad assumption, the performance of a FCN is not a great compared to a neural network that is designed for a specific kind of input, like a CNN. Another advantage is that FCNs have more expressive power compared to CNNs due to convolution being linear.\n",
    "\n",
    "The specific focus that a CNN is designed for is quite useful as one can process the input of images quite quickly compared to a FCN. However, the main disadvantage of this type of neural network is that you can only train the network on images and nothing else which is where the FCN comes in. Another advantage is that CNNs seem to be more efficient in utilizing their parameters. FCNs tend to require a greater number of parameters to compete to an equivalent CNN.\n",
    "\n",
    "So, overall, depending on your needs, a FCN can be better than a CNN and vice versa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40f6d341",
   "metadata": {},
   "source": [
    "What are some ways that FCNs can be used?\n",
    "\n",
    "In searching for good real world examples of FCNs as a way to better explain FCNs, I came across three different papers that peaked my interest. \n",
    "\n",
    "The first one is called Intra Prediction using Fully Connected Network for Video Coding which is by Jihao Li, Bin Li, Jizheng Xu, and Ruiqin Xiong. \n",
    "\n",
    "The second one is called Fully Connected Network on Noncompact Symmetric Space and Ridgelet Transform based on Helgason Fourier Analysis which is by Sho Sonoda, Isao Ishikawa, and Masahiro Ikeda.\n",
    "\n",
    "The third one is called How Far Can We go Without Convolution: Improving Fully Connected Networks which is by Zhouhan Lin, Roland Memisevic, and Kishore Konda.\n",
    "\n",
    "I will go over the three world examples after my example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55279671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab38454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358ebf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977550a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 784  # MNIST image size: 28x28 = 784\n",
    "hidden_size = 128\n",
    "output_size = 10  # Number of classes in MNIST dataset\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4368c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train(model, train_loader, optimizer, criterion, num_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "train(model, train_loader, optimizer, criterion, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
