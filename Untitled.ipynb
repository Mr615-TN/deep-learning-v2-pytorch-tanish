{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3949fce7",
   "metadata": {},
   "source": [
    "What is a fully connected network (FCN for short)?\n",
    "\n",
    "A fully connected network is a type of artifical neural network whose structure consists of fully connected layers. Fully connected layers are layers that are connected to other layers by the neurons/nodes in each layer. For example, if there is a FCN that has 3 layers with layer 1 having 10 neurons, layer 2 having 26 neurons, and layer 3 having 14 neurons. In this example, each one of the neurons in layer 1 has a connection to every single neuron in layer 2 and the same concept holds true for the relation between layer 2 and layer 3. What is also interesting is that you can figure out the amount of connections that are there between two layers. The equation for that the N1*N2 where N1 is the number of neurons in the layer on the left and N2 is the number of neurons on the right.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3188284",
   "metadata": {},
   "source": [
    "How do Fully Connected Networks work?\n",
    "\n",
    "FCNs work by having a neuron/perceptron apply a linear transformation to the input vector through a weight matrix. Then a non-linear transformation is applied to the product of the input vector and weight matrix through a non linear activation function (show equation image).\n",
    "\n",
    "Basically, we are taking the dot product of the weight matrix W and the input vector x. Then the bias term W0 will be added inside the non linear function. (A Bias term is a disproportionate weight in favor or against an idea or thing). In even simpler terms, we are doing vector multiplication. For example, we have an input vector of 1x9 and a weight matrix of 9x4. We will take the dot product of (1x9) and (9x4) and then apply the non-linear transformation with the activation function f to get an output vector of (1x4) (show the second image before the example and the third fcl image after the example).\n",
    "\n",
    "An Activation function is a model that defines the output of a neuron/node given an input or set of input.\n",
    "\n",
    "Some of the most commonly used activation functions are:\n",
    "Binary,\n",
    "Linear,\n",
    "Sigmoid,\n",
    "Tanh,\n",
    "ReLU,\n",
    "Leaky ReLU,\n",
    "Parametric ReLU,\n",
    "Exponential Linear Unit,\n",
    "ReLU-6,\n",
    "Softplus,\n",
    "Softsign, \n",
    "Softmax, \n",
    "and Swish\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66e36a85",
   "metadata": {},
   "source": [
    "How does a FCN differ from a CNN?\n",
    "\n",
    "The biggest difference from what I seen online in doing my own research is that FCNs are structurally agnostic meaning that they don't make any special assumption about the input given whereas a CNN is designed to assume that the input are specifically images. \n",
    "\n",
    "This broad assumption that FCNs have can be quite useful if one wants to train different data. However, due to the broad assumption, the performance of a FCN is not a great compared to a neural network that is designed for a specific kind of input, like a CNN. Another advantage is that FCNs have more expressive power compared to CNNs due to convolution being linear.\n",
    "\n",
    "The specific focus that a CNN is designed for is quite useful as one can process the input of images quite quickly compared to a FCN. However, the main disadvantage of this type of neural network is that you can only train the network on images and nothing else which is where the FCN comes in. Another advantage is that CNNs seem to be more efficient in utilizing their parameters. FCNs tend to require a greater number of parameters to compete to an equivalent CNN.\n",
    "\n",
    "So, overall, depending on your needs, a FCN can be better than a CNN and vice versa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40f6d341",
   "metadata": {},
   "source": [
    "What are some ways that FCNs can be used?\n",
    "\n",
    "In searching for good real world examples of FCNs as a way to better explain FCNs, I came across three different papers that peaked my interest. \n",
    "\n",
    "The first one is called Intra Prediction using Fully Connected Network for Video Coding which is by Jihao Li, Bin Li, Jizheng Xu, and Ruiqin Xiong. \n",
    "\n",
    "The second one is called Fully Connected Network on Noncompact Symmetric Space and Ridgelet Transform based on Helgason Fourier Analysis which is by Sho Sonoda, Isao Ishikawa, and Masahiro Ikeda.\n",
    "\n",
    "The third one is called How Far Can We go Without Convolution: Improving Fully Connected Networks which is by Zhouhan Lin, Roland Memisevic, and Kishore Konda.\n",
    "\n",
    "I will go over the three world examples after my example code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a571b589",
   "metadata": {},
   "source": [
    "The goal of my example is to train my FCN over a dataset of 10000 different images by using the MNIST dataset.\n",
    "\n",
    "In my example I am creating a FCN using the nn.Module class which consists of two fully connected layers with ReLu activation in between the two layers.\n",
    "\n",
    "I also defined the forwward pass function to help compute the output of the model.\n",
    "\n",
    "During the training of the FCN, the code should iterate over the training dataset in batches and also perform forward and backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55279671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab38454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fully connected neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad857b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 784  # Input size of MNIST dataset (28x28 pixels)\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ebf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the neural network\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977550a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4368c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the neural network\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {(100 * correct / total):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94b012e1",
   "metadata": {},
   "source": [
    "A Brief Summary of Intra Prediction using Fully Connected Network for Video Coding which is by Jihao Li, Bin Li, Jizheng Xu, and Ruiqin Xiong:\n",
    "\n",
    "Basically, what Dr. J Li, Dr. B Li, Dr. J Xu, and Dr. R Xiong wanted to achieve was to successfully apply Deep Neural Networks through FCNs to hopefully improve the SOTA intra prediction. The frame work that the four gentlemen took into consideration block-based adn they proposed using a FCN where all layers except non linear layers are fully connected.\n",
    "\n",
    "The method that Dr Lis, Dr. Xu, and Dr. Xiong suggested in comparesion to the traditional method actually exploits a richer context of current blocks.\n",
    "\n",
    "Currently the SOTA of video coding standard is HEVC or High Efficiency Video Coding. The four gentlemen were able to improve it by 1.1%.\n",
    "\n",
    "Also, Dr. J Li, Dr. B Li, Dr. J Xu, and Dr. R Xiong were able to improve 4k by 1.6%. \n",
    "\n",
    "In the end, Dr. J Li, Dr. B Li, Dr. J Xu, and Dr. R Xiong were able to find out that intra prediction can be optimized for video coding by using the suggested version of a fully connected network. These four men found that a 128-dimensional IPFCN with 3 layers is the best way for video coding. They do state that the training set contained 48 sets and block size was 8x8 and they hoped to investigate this in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67adcdec",
   "metadata": {},
   "source": [
    "\n",
    "A Brief Summary of Fully Connected Network on Noncompact Symmetric Space and Ridgelet Transform based on Helgason Fourier Analysis which is by Sho Sonoda, Isao Ishikawa, and Masahiro Ikeda:\n",
    "\n",
    "Basically, what Dr. Sonoda, Dr. Ishikawa, and Dr. Ikeda wanted to accomplish is to present a fully connected network and its associated ridgeliet transform on a noncompact symmetric space using the framework of the Helgason-Fourier transform on a noncompact symmetric space.\n",
    "\n",
    "A symmetric space is a Riemannian manifold (a manifold that is equipped with a postive inner product at each point of a tangent space) whose groupe of symmetries contains an inversion symmetry at every single point. A noncompact symmetric space is a symmetric space that has nonpostive sectional curvature (a way to describe the curve of a Riemannian manifold)\n",
    "\n",
    "The Helgason Fourier Transform is a mathematical model used to help to transform signals/inputs between two different domains. An example of this is transfroming signals from a frequency domain to a time domain or vice versa. The Helgason Fourier transform is more specifically applied to noncompact Riemannian symmetric spaces.\n",
    "\n",
    "A ridgelet transfrom is a right inversion operator of the intgeral representation of the operator S. (show pictures)\n",
    "\n",
    "In the end, Dr. Sonoda, Dr. Ishikawa, and Dr. Ikeda were able to devise a fully connected layter on a non compact space and were able to present it on a closed form expression of a ridgelet transform. The three gentlemen go on furthur to state that given nice coordinates, they could turn it into a Fourier Expression and then maybe obtain a ridgelet transform from the coordinates. This is because what Dr. Sonoda, Dr. Ishikawa, and Dr. Ikeda did was similar to HNNs (hyperbolic neural networks which are specialized NNs for these kind of problems that Dr. Sonoda, Dr. Ishikawa, and Dr. Ikeda are trying to work on)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6667363",
   "metadata": {},
   "source": [
    "A Brief Summary of How Far Can We go Without Convolution: Improving Fully Connected Networks which is by Zhouhan Lin, Roland Memisevic, and Kishore Konda:\n",
    "\n",
    "Basically, what Dr. Lin, Dr. Memisevic, and Dr. Konda wanted to accomplish was to improve the performance of fully connected networks and these three proposed two approaches that actually improve performance: linear bottlenecks layers and unsupervised pre-training with autoencoders.\n",
    "\n",
    "A big advantage of linear bottleneck layers is that it counteracts the issue of sparscity in neural networks. The drawback of sparscity is that there will be a scarcity of data. However, by using linear bottleneck layers in FCNs is that the amount of data can increase, decrease, or stay the same and not have to deal with sparscity.\n",
    "\n",
    "A big advantage of pre training with autoencoders is that the weight matrices are closer to the orthogonal and are less likely by vanishing gradient problems (what this means is that the value of the product of deriivate decreases until at some point, the partial derivate reaches a value close to 0 or actually hit 0 and then the partial derivate will disappear).\n",
    "\n",
    "Ultimately, Dr. Lin, Dr. Memisevic, and Dr. Konda were able to improve the performance of FCNs through the discussed methods above. This also in part that linear bottleneck layers and pre-training with autoencoders have been an idea and somewhat used for a long period of time. However, the only downside is that the practicality of the improved performance is limited as to approximate any given function requires an extremely large number of hidden units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
